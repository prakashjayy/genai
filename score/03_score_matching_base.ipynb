{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## score matching\n",
    "\n",
    "consider u have n data points of unknown distribution and if u want to build a generative model to sample new data points. there are several approaches to solve this problem. One way could be to estimate the pdf of the distribution. \n",
    "\n",
    "$$\n",
    "p(x) = \\frac{f(x)}{\\int f(x)dx}\n",
    "$$\n",
    "\n",
    "here p is defined as probabilty of x occuring.\n",
    "\n",
    "Where:\n",
    "- f(x) is an unnormalized density function\n",
    "- p(x) is the properly normalized probability density function\n",
    "\n",
    "the integral in the denomintor is difficult to calculate for complex data like images and text. A 32x32x3 image has 3072 dimensions and calcuating the denomintor $\\int f(x_{1}, x_{2}, ... x_{n})dx_{1} dx_{2}... dx_{n}$. there are no analytical approaches to solve this and we generally call this `intractable`. \n",
    "\n",
    "\n",
    "if we try to model this using some parameters then the equation becomes \n",
    "\n",
    "$$\n",
    "p(x ; \\theta) = \\frac{f(x; \\theta)}{\\int f(x; \\theta)dx}\n",
    "$$\n",
    "\n",
    "if we take log on both sides \n",
    "\n",
    "$$\n",
    "\\log{p(x; \\theta)} = \\log(f(x;\\theta)) - \\log(\\int f(x; \\theta)dx)\n",
    "$$\n",
    "\n",
    "Integration eliminates the integration variable, so our \\int f(x, \\theta)dx can be re-written as $Z_{\\theta}$ which is free of x now.  \n",
    "\n",
    "$$\n",
    "\\log{p(x;\\theta)} = \\log(f(x;\\theta)) - \\log(Z_{\\theta})\n",
    "$$\n",
    "\n",
    "if we take derivative with x \n",
    "\n",
    "$$\n",
    "\\triangledown_{x} \\log{p(x;\\theta)} = \\triangledown_{x} \\log(f(x, \\theta)) - \\triangledown_{x} \\log(Z_{\\theta})\n",
    "$$\n",
    "\n",
    "the last term when integrating with x is zero, so \n",
    "\n",
    "$$\n",
    "\\triangledown_{x} \\log{p(x;\\theta)} = \\triangledown_{x} \\log(f(x, \\theta)) \n",
    "$$\n",
    "\n",
    "this is defined as $\\triangledown_{x} \\log(f(x, \\theta))$ as $s_{\\theta}(x)$ is a neural network which takes x as input and outputs the gradient vector field at that particular point. \n",
    "\n",
    "How do we optimize this ? so the network outputs a vector field (model score) and original data has a vector field (data score). we can take the difference of each of these points and then average. if both the scores are same we will have zero loss. this is exactly what is captured by fisher divergence.  \n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\mathbb{E}_{p_{\\text{data}}(\\mathbf{x})} \\left[ \\left\\| \\nabla_{\\mathbf{x}} \\log p_{\\text{data}}(\\mathbf{x}) - s_{\\theta}(\\mathbf{x}) \\right\\|^2_2 \\right]\n",
    "$$\n",
    "\n",
    "However we don't know the ground truth value of the data-score function. there is a way to achieve this using `score matching` described by hyvarinen [here](https://andrewcharlesjones.github.io/journal/21-score-matching.html). \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{p_{\\text{data}}(\\mathbf{x})} \\left[ \\frac{1}{2} \\|s_{\\theta}(\\mathbf{x})\\|^2_2 + \\text{trace}( \\underbrace{\\nabla_{\\mathbf{x}} s_{\\theta}(\\mathbf{x})}_{\\text{Jacobian of } s_{\\theta}(\\mathbf{x})} ) \\right]\n",
    "$$\n",
    "\n",
    "- the first term is simple the sum of squared terms of the output. \n",
    "- in order to obtain second term, we need to calculate the jacobbian matrix between inputs and outputs. so if we have 3072 inputs and 3072 outputs the jacobian matrix [3072x3072] matrix. taking the sum of all diagonal elements would give u the value of 2nd term.  However getting this matrix is extremely time consuming as we need to calculate gradient 3072 times. there are approximations for this but still for large images of size 512x512 this will be very time-consuming and resource intensive. \n",
    "\n",
    "\n",
    "lets take a toy example and compute it. we have a 2D input and we want to fit a linear model to it with output having sigmoid.\n",
    "\n",
    "## 1st layer \n",
    "$$\n",
    "\\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix} = \\begin{bmatrix} w_{11} & w_{12} \\\\ w_{21} & w_{22} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    " \\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix} = \\begin{bmatrix} w_{11} x_1 + w_{21} x_2 + b_1 \\\\ w_{12} x_1 + w_{22} x_2 + b_2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## 2nd layer \n",
    "$$\n",
    "\\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix} = \\begin{bmatrix} \\sigma(y_1) \\\\ \\sigma(y_2) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Now lets calculate the score function $s_\\theta(x)$ aka jacobian of the output with respect to the input.\n",
    "\n",
    "$$\n",
    "J = \\begin{bmatrix}\n",
    "\\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} \\\\\n",
    "\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now lets calculate the gradient for each element. In the above the first element is \n",
    "\n",
    "$$\n",
    "y1 = \\sigma(w_{11} x_1 + w_{21} x_2 + b_1)\n",
    "$$\n",
    "\n",
    "as we know the derivative of sigmoid is \n",
    "\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_1}{\\partial x_1} = \\sigma (w_{11} x_1 + w_{21} x_2 + b_1) (1 - \\sigma (w_{11} x_1 + w_{21} x_2 + b_1)) w_{11}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_2}{\\partial x_2} = \\sigma (w_{12} x_1 + w_{22} x_2 + b_2) (1 - \\sigma (w_{12} x_1 + w_{22} x_2 + b_2)) w_{22}\n",
    "$$\n",
    "\n",
    "\n",
    "Now lets take some values and calculate this . we will take w_11 = 1, w_12 = 2, w_21 = 3, w_22 = 4, b_1 = 0, b_2 = 0, x_1 = 1, x_2 = 1\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_1}{\\partial x_1} = \\sigma (1 + 3 + 0) (1 - \\sigma (1 + 3 + 0)) 1 = 0.017\n",
    "$$\n",
    "\n",
    "we will see in pytorch how to calculate this. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score output: tensor([[0.9820, 0.9975]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Simple score network\n",
    "class ToyScoreNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 2)  # 2D input -> 2D output\n",
    "        self.linear.weight.data = torch.tensor([[1, 3], [2, 4]]).float()\n",
    "        self.linear.bias.data = torch.tensor([0, 0]).float()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))  # Returns score: [∂/∂x₁ log p, ∂/∂x₂ log p]\n",
    "\n",
    "# Create network and sample data\n",
    "score_net = ToyScoreNetwork()\n",
    "x = torch.tensor([[1.0, 1.0]], requires_grad=True)  # One 2D point\n",
    "\n",
    "# Method 1: Computing full gradients\n",
    "score = score_net(x)  # Shape: [1, 2]\n",
    "print(\"Score output:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0177, 0.0530]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(\n",
    "        score[:, 0],  # Take i-th component\n",
    "        x,\n",
    "        create_graph=True\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0049, 0.0099]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(\n",
    "        score[:, 1],  # Take i-th component\n",
    "        x,\n",
    "        create_graph=True\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As u can see 0.0177 matches with the calculating the gradient of the function wrt to the input.\n",
    "\n",
    "Now we will implement our first score matching network here and see how the network learns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
