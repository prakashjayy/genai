{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score matching \n",
    "\n",
    "we have data $p_{d}(x)$ and since this is diffcult to model directly, we want to fit a model $p_{m}(x, \\theta)$. Here $\\theta$ is the parameter of the model. As we train the model, we want to match the data distribution $p_{d}(x)$ as closely as possible.\n",
    "\n",
    "we can write $p_{m}(x, \\theta)$ as:\n",
    "\n",
    "$$\n",
    "p_m(x;\\theta) = \\frac{\\tilde{p}(x;\\theta)}{Z_\\theta}, \\quad Z_\\theta = \\int_\\mathcal{X} \\tilde{p}(x;\\theta)dx.\n",
    "$$\n",
    "\n",
    "\n",
    "where $\\tilde{p}$ is the unnormalized model distribution. Now since z is intractable we can remove this by derivating the above equation with respect to x \n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{x}} \\log p_m(x;\\theta) = \\nabla_{\\mathbf{x}} \\log \\tilde{p}(x;\\theta)  + \\nabla_{\\mathbf{x}} \\log Z_\\theta\n",
    "$$\n",
    "\n",
    "and since $z_\\theta$ is independent of x, we can remove it.\n",
    "\n",
    "$$\n",
    "\\nabla_{\\mathbf{x}} \\log p_m(x;\\theta) = \\nabla_{\\mathbf{x}} \\log \\tilde{p}(x;\\theta)\n",
    "$$\n",
    "\n",
    "so even though it is not the model distribution in its original form, score function gives the gradient of the model distribution. So here we will match the gradient of the model distribution with the gradient of the data distribution. Using fisher divergence we can write the difference between the two gradients as:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2}\\mathbb{E}_{p_d}\\left[\\|\\nabla_x \\log p_d(x) - \\nabla_x \\log p_m(x;\\theta)\\|_2^2\\right].\n",
    "$$\n",
    "\n",
    "and minimizing this is important to us. but computing this is difficult. The authors of the paper derived and wrote this in a simpler form as:\n",
    "\n",
    "$$\n",
    "L(\\theta) \\approx \\frac{1}{n}\\sum_{i=1}^n\\left[\\text{tr}\\left(\\nabla_x^2\\log p_m(x_i;\\theta)\\right) + \\frac{1}{2}\\|\\nabla_x\\log p_m(x_i;\\theta)\\|_2^2\\right]\n",
    "$$\n",
    "\n",
    "A detailed blog is [here]( https://andrewcharlesjones.github.io/journal/21-score-matching.html) on how we have arrived at this equation is here.\n",
    "\n",
    "say we have a neural network model which takes an image as input and outputs a scalar score for each pixel.  then\n",
    "- we can obtain the 2nd term by simply passing image into a neural network and square each value and sum them up. \n",
    "- 1st term is taking double graident, 1st gradient is obtained in the forward pass as above and the 2nd gradient is obtained by taking gradient of the gradient. This is called Hessian matrix for confusion but this is just jaccobian matrix when we consider this as derivative of output with respect to input. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will work on a toy example to understand this better.  Lets consider we have a 2D input and we want to fit a linear model to it with output having sigmoid.\n",
    "\n",
    "## 1st layer \n",
    "$$\n",
    "\\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix} = \\begin{bmatrix} w_{11} & w_{12} \\\\ w_{21} & w_{22} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    " \\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix} = \\begin{bmatrix} w_{11} x_1 + w_{21} x_2 + b_1 \\\\ w_{12} x_1 + w_{22} x_2 + b_2 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## 2nd layer \n",
    "$$\n",
    "\\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix} = \\begin{bmatrix} \\sigma(y_1) \\\\ \\sigma(y_2) \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Now lets calculate the gradient for each element. In the above the first element is \n",
    "\n",
    "$$\n",
    "y1 = \\sigma(w_{11} x_1 + w_{21} x_2 + b_1)\n",
    "$$\n",
    "\n",
    "as we know the derivative of sigmoid is \n",
    "\n",
    "$$\n",
    "\\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_1}{\\partial x_1} = \\sigma (w_{11} x_1 + w_{21} x_2 + b_1) (1 - \\sigma (w_{11} x_1 + w_{21} x_2 + b_1)) w_{11}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_2}{\\partial x_2} = \\sigma (w_{12} x_1 + w_{22} x_2 + b_2) (1 - \\sigma (w_{12} x_1 + w_{22} x_2 + b_2)) w_{22}\n",
    "$$\n",
    "\n",
    "\n",
    "Now lets take some values and calculate this . we will take w_11 = 1, w_12 = 2, w_21 = 3, w_22 = 4, b_1 = 0, b_2 = 0, x_1 = 1, x_2 = 1\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y_1}{\\partial x_1} = \\sigma (1 + 3 + 0) (1 - \\sigma (1 + 3 + 0)) 1 = 0.017\n",
    "$$\n",
    "\n",
    "we will see in pytorch how to calculate this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score output: tensor([[0.9820, 0.9975]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Simple score network\n",
    "class ToyScoreNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 2)  # 2D input -> 2D output\n",
    "        self.linear.weight.data = torch.tensor([[1, 3], [2, 4]]).float()\n",
    "        self.linear.bias.data = torch.tensor([0, 0]).float()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))  # Returns score: [∂/∂x₁ log p, ∂/∂x₂ log p]\n",
    "\n",
    "# Create network and sample data\n",
    "score_net = ToyScoreNetwork()\n",
    "x = torch.tensor([[1.0, 1.0]], requires_grad=True)  # One 2D point\n",
    "\n",
    "# Method 1: Computing full gradients\n",
    "score = score_net(x)  # Shape: [1, 2]\n",
    "print(\"Score output:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0177], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.grad(\n",
    "        score[:, 0],  # Take i-th component\n",
    "        x,\n",
    "        create_graph=True\n",
    "    )[0][:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so if u see 0.017 term matches up with our original calculation. to calculate the overall loss we sum over the two terms and do the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagonal element 0: tensor([0.0177], grad_fn=<SelectBackward0>)\n",
      "Diagonal element 1: tensor([0.0099], grad_fn=<SelectBackward0>)\n",
      "loss tensor([0.0275], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "div = 0 \n",
    "for i in range(2):  # For each dimension\n",
    "    grad_i = torch.autograd.grad(\n",
    "        score[:, i].sum(),  # Take i-th component\n",
    "        x,\n",
    "        create_graph=True\n",
    "    )[0][:, i]  # Take i-th diagonal element\n",
    "    div += grad_i\n",
    "    print(f\"Diagonal element {i}:\", grad_i)\n",
    "print(\"loss\", div)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we got everything we are looking for, now lets train a model using MNIST dataset. \n",
    "\n",
    "> Note: In the above code, gradient is calculated with respect to each output score value. for a 32x32 image, we have 1024 output scores. so we will have 1024 gradients passing. This can be quite slow and time consuming. \n",
    "\n",
    "> So there is another paper called [sliced score matching] to solve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0216, -0.7956]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
