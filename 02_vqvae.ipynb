{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VQ-VAE\n",
    "vector quantisation variational autoencoder allows you to learn discrete latent codes from data. Language is inherently discrete [1](https://arxiv.org/pdf/1711.00937), similarly speech is typically represented as a sequence of symbols. Images can often be described concisely by language.\n",
    "\n",
    "\n",
    "So the understanding is that we will have fixed codes to map from high dimension to low dimension. Lets take an image x and encode to $z_{e}$ using an encoder p(z/x). Now using L2 distance we will select the closest code $z_{q}$ to $z_{e}$. Now we will decode $z_{q}$ to get $x_{q}$ using decoder p(x/z).\n",
    "\n",
    "$$\n",
    "    x \\overset{\\text{encoder}}{\\longrightarrow} z_{e} \\overset{\\text{quantisation}}{\\longrightarrow} z_{q} \\overset{\\text{decoder}}{\\longrightarrow} x_{q}\n",
    "$$\n",
    "\n",
    "so if u have a image of dimension 32x32x3, we can encode it to 512 dimension ($z_{e} \\in \\mathbb{R}^{512}$) and have $z_{q} \\in \\mathbb{R}^{1024}$ codes, with each code being 512, we will calculate L2 distance between $z_{e}$ and all the codes and select the one with the least distance, use this code to decode to $x_{q}$. Now the fundamental question i have is say if u have 50k images, how are we encoding them to just 1024 codes? and how can learn/reconstruct all the images from just 1024 codes? \n",
    "\n",
    "## Codebook size\n",
    "Input Image: 32x32x3 (RGB image)\n",
    "Encoded Representation: 4x4x512\n",
    "Codebook Size: 1024 codes (each code is 512-dimensional)\n",
    "\n",
    "The encoder produces 16 vectors (4x4=16) where each vector is 512-dimensional. Each of these 16 vectors gets mapped to its nearest neighbor in the codebook. This means:\n",
    "- Each spatial location in the 4x4 encoded space selects one code from the codebook\n",
    "- We need 16 selections from the codebook to represent one image\n",
    "- Total possible combinations = 1024¹⁶ (or 2⁵¹²) unique images\n",
    "- This massive space (2⁵¹²) easily accommodates our 50k training images\n",
    "\n",
    "## Compression Analysis\n",
    "VQ-VAE performs lossy compression. Here's why:\n",
    "\n",
    "Original Image Size:\n",
    "- Dimensions: 32x32x3 pixels\n",
    "- Bits per pixel: 8 (0-255)\n",
    "- Total bits: 32 × 32 × 3 × 8 = 24,576 bits\n",
    "\n",
    "Compressed Representation:\n",
    "- Need to store 16 indices (4x4 spatial locations)\n",
    "- Each index needs 10 bits (to represent 1024 choices)\n",
    "- Total bits: 16 × 10 = 160 bits\n",
    "\n",
    "Compression ratio = 24,576/160 = 153.6x reduction in size\n",
    "\n",
    "\n",
    "## How to learn the codebook?\n",
    "The VQ_VAE paper using the following loss function to learn the codebook.\n",
    "\n",
    "$$\n",
    "    L = ||x - D[e(x)]||^2 + ||sg[e(x)] - C||^2 + \\beta ||sg[C] - e(x)||^2\n",
    "$$\n",
    "\n",
    "where $D$ is the decoder, $e$ is the encoder, $C$ is the codebook, $sg$ is the stop gradient operation, $\\beta$ is a scalar hyper parameter.\n",
    "The first term is the reconstruction loss, the second term is the codebook loss and the third term is the commitment loss. The encoder and decoder are trained to minimise the reconstruction loss, while the codebook is updated to minimise the codebook loss and commitment loss. \n",
    "\n",
    "\n",
    "## Commitment loss\n",
    "The commitment loss is used as a regularizer to ensure that both $z_{e}$ and $z_{q}$ are not changed simultaneously which makes training unstable. Lets take an example again.\n",
    "- Iteration one - image1 has selected [10, 20] code book indices and $z_{e}$ is [0.1, 0.2]\n",
    "- Iteration two - image1 has selected [30, 40] code book indices and $z_{e}$ is [0.15, 0.18]\n",
    "so in the above case both $z_{e}$ and $z_{q}$ are changed simultaneously. To avoid this we use the commitment loss, where we keep the codebook fixed (hence the stop gradient operation so that it does not backpropagate and change the codebook) and try to push $z_{e}$ to be close to $z_{q}$. fixing the codebook makes more sense as ultimately code book learning useful features from the data is our main priority. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24576"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*32*3*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
